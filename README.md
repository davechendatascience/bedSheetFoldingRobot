# åºŠå–®æ‘ºç–Šæ©Ÿå™¨äºº - é—œéµé»æª¢æ¸¬

ä¸€å€‹ä½¿ç”¨æ·±åº¦å­¸ç¿’å’Œé›»è…¦è¦–è¦ºæŠ€è¡“çš„åºŠå–®æ‘ºç–Šæ©Ÿå™¨äººé—œéµé»æª¢æ¸¬ç³»çµ±ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹è‰²

- **æ··åˆé—œéµé»æª¢æ¸¬æ¨¡å‹**ï¼šYOLO + Vision Transformer æ¶æ§‹
- **å…©éšæ®µè¨“ç·´æµç¨‹**ï¼šé è¨“ç·´ + å¾Œè¨“ç·´å„ªåŒ–
- **å„ªåŒ–è¨“ç·´æµç¨‹**ï¼šä½¿ç”¨ `torch.compile()`ã€æ—©åœæ©Ÿåˆ¶å’Œ mixup å¢å¼·
- **å³æ™‚æ¨ç†**ï¼šé‡å°å³æ™‚åºŠå–®é—œéµé»æª¢æ¸¬å„ªåŒ–
- **å…¨é¢æ•¸æ“šè™•ç†**ï¼šYOLO åˆ†å‰² + é—œéµé»è¨»è§£æµç¨‹
- **åˆæˆæ•¸æ“šç”Ÿæˆ**ï¼šä½¿ç”¨ Blender ç”Ÿæˆè¨“ç·´æ•¸æ“š

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
bedSheetFoldingRobot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/                 # æ¨¡å‹æ¶æ§‹
â”‚   â”‚   â”œâ”€â”€ hybrid_keypoint_net.py
â”‚   â”‚   â””â”€â”€ efficient_keypoint_net.py
â”‚   â”œâ”€â”€ utils/                  # å·¥å…·å‡½æ•¸
â”‚   â”‚   â”œâ”€â”€ model_utils.py      # YOLO éª¨å¹¹å’Œæ¨¡å‹å·¥å…·
â”‚   â”‚   â””â”€â”€ tensorrt_utils.py   # TensorRT è½‰æ›å·¥å…·ï¼ˆæœªä¾†ï¼‰
â”‚   â””â”€â”€ training/               # è¨“ç·´æµç¨‹
â”œâ”€â”€ cloth_data_gen/             # åˆæˆæ•¸æ“šç”Ÿæˆ
â”‚   â”œâ”€â”€ cloth_dataset_gen.py    # Blender æ•¸æ“šç”Ÿæˆè…³æœ¬
â”‚   â””â”€â”€ output/                 # ç”Ÿæˆçš„æ•¸æ“šè¼¸å‡ºç›®éŒ„
â”œâ”€â”€ realsense/                  # RealSense å·¥å…·é›†
â”œâ”€â”€ shared/                     # å…±äº«å‡½æ•¸å’Œå·¥å…·
â”œâ”€â”€ models/                     # è¨“ç·´æ¨¡å‹å’Œ YOLO æ¬Šé‡
â”œâ”€â”€ data/                       # æ•¸æ“šé›†å’Œè¨»è§£
â”œâ”€â”€ results/                    # è¨“ç·´çµæœå’Œè¦–è¦ºåŒ–
â”œâ”€â”€ keypoint_detection_model_training.py  # ç¬¬ä¸€éšæ®µï¼šé è¨“ç·´è…³æœ¬
â”œâ”€â”€ post_keypoint_detection_model_training.py  # ç¬¬äºŒéšæ®µï¼šå¾Œè¨“ç·´è…³æœ¬
â”œâ”€â”€ convert_to_tensorrt.py      # TensorRT è½‰æ›è…³æœ¬ï¼ˆæœªä¾†ï¼‰
â””â”€â”€ test_tensorrt_inference.py  # æ•ˆèƒ½åŸºæº–æ¸¬è©¦ï¼ˆæœªä¾†ï¼‰
```

## ğŸ› ï¸ å®‰è£

1. **è¤‡è£½å„²å­˜åº«ï¼š**
```bash
git clone <repository-url>
cd bedSheetFoldingRobot
```

2. **å®‰è£ä¾è³´ï¼š**
```bash
pip install -r requirements.txt
```

3. **å®‰è£ Blenderï¼ˆç”¨æ–¼æ•¸æ“šç”Ÿæˆï¼‰ï¼š**
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install blender

# æˆ–å¾å®˜ç¶²ä¸‹è¼‰ï¼šhttps://www.blender.org/download/
```

## ğŸ¯ å¿«é€Ÿé–‹å§‹

### æ­¥é©Ÿ 0ï¼šç”Ÿæˆè¨“ç·´æ•¸æ“šï¼ˆå¯é¸ï¼‰

å¦‚æœæ‚¨éœ€è¦ç”Ÿæˆåˆæˆè¨“ç·´æ•¸æ“šï¼Œå¯ä»¥ä½¿ç”¨ Blender è…³æœ¬ï¼š

```bash
# é€²å…¥æ•¸æ“šç”Ÿæˆç›®éŒ„
cd cloth_data_gen

# ä½¿ç”¨ Blender åŸ·è¡Œæ•¸æ“šç”Ÿæˆè…³æœ¬
blender --background --python cloth_dataset_gen.py
```

#### æ•¸æ“šç”Ÿæˆé…ç½®
è…³æœ¬æœƒç”Ÿæˆä»¥ä¸‹å…§å®¹ï¼š
- **3000 å€‹æ¨£æœ¬**ï¼ˆå¯èª¿æ•´ `n_samples` åƒæ•¸ï¼‰
- **128x128 åƒç´ åœ–åƒ**
- **4 å€‹è§’é»é—œéµé»**
- **éš¨æ©Ÿè®Šå½¢å’Œé¡è‰²**
- **è¼¸å‡ºç›®éŒ„**ï¼š`output/images/` å’Œ `output/keypoints/`

#### è‡ªè¨‚ç”Ÿæˆåƒæ•¸
ç·¨è¼¯ `cloth_dataset_gen.py` ä¸­çš„åƒæ•¸ï¼š
```python
n_samples = 3000          # ç”Ÿæˆæ¨£æœ¬æ•¸é‡
length_range = (3.5, 5.0) # å¸ƒæ–™é•·åº¦ç¯„åœ
width_range = (3.5, 5.0)  # å¸ƒæ–™å¯¬åº¦ç¯„åœ
res = 40                  # ç¶²æ ¼ç´°åˆ†æ•¸
```

### ç¬¬ä¸€éšæ®µï¼šé è¨“ç·´

é¦–å…ˆï¼Œæ‚¨éœ€è¦ä½¿ç”¨åŸå§‹è¨“ç·´è…³æœ¬é è¨“ç·´æ¨¡å‹ï¼š

```bash
# ç¬¬ä¸€éšæ®µï¼šé è¨“ç·´æ¨¡å‹
python keypoint_detection_model_training.py
```

é€™å°‡æœƒï¼š
- åœ¨æ‚¨çš„æ•¸æ“šé›†ä¸Šå¾é ­é–‹å§‹è¨“ç·´æ¨¡å‹
- å°‡é è¨“ç·´æ¨¡å‹å„²å­˜åˆ° `models/keypoint_model_vit.pth`
- å»ºç«‹åŸºæº–æ•ˆèƒ½

### ç¬¬äºŒéšæ®µï¼šå¾Œè¨“ç·´å„ªåŒ–

é è¨“ç·´å¾Œï¼Œä½¿ç”¨å¾Œè¨“ç·´è…³æœ¬é€²è¡Œå„ªåŒ–ï¼š

```bash
# ç¬¬äºŒéšæ®µï¼šå¾Œè¨“ç·´å„ªåŒ–
python post_keypoint_detection_model_training.py config_quantization_fixed.json
```

**é…ç½®é¸é …ï¼š**
- `num_epochs`ï¼šè¨“ç·´è¼ªæ•¸ï¼ˆé è¨­ï¼š50ï¼‰
- `batch_size`ï¼šæ‰¹æ¬¡å¤§å°ï¼ˆé è¨­ï¼š16ï¼‰
- `learning_rate`ï¼šå­¸ç¿’ç‡ï¼ˆé è¨­ï¼š0.001ï¼‰
- `early_stopping_patience`ï¼šæ—©åœè€å¿ƒå€¼ï¼ˆé è¨­ï¼š10ï¼‰
- `use_mixup`ï¼šå•Ÿç”¨ mixup å¢å¼·ï¼ˆé è¨­ï¼štrueï¼‰

## ğŸ—ï¸ æ¨¡å‹æ¶æ§‹

### æ··åˆé—œéµé»ç¶²è·¯
- **éª¨å¹¹**ï¼šYOLO11L-poseï¼ˆå‰12å±¤ï¼‰
- **é ­éƒ¨**ï¼šç”¨æ–¼é—œéµé»æª¢æ¸¬çš„ Vision Transformer
- **è¼¸å‡º**ï¼šåŸºæ–¼ç†±åŠ›åœ–çš„é—œéµé»é æ¸¬
- **è¼¸å…¥**ï¼š128x128 RGB åœ–åƒ
- **åƒæ•¸**ï¼šç´„100M åƒæ•¸

### ä¸»è¦ç‰¹è‰²
- **torch.compile()**ï¼šä½¿ç”¨ PyTorch 2.0 ç·¨è­¯å„ªåŒ–è¨“ç·´
- **æ—©åœæ©Ÿåˆ¶**ï¼šåŸºæ–¼é©—è­‰æå¤±å¹³å°çš„è‡ªå‹•è¨“ç·´çµ‚æ­¢
- **Mixup å¢å¼·**ï¼šä½¿ç”¨ mixup æ•¸æ“šå¢å¼·æ”¹å–„æ³›åŒ–èƒ½åŠ›
- **æœ€ä½³æ¨¡å‹å„²å­˜**ï¼šåŸºæ–¼é©—è­‰æå¤±è‡ªå‹•å„²å­˜æœ€ä½³æ¨¡å‹

## ğŸ“Š æ•ˆèƒ½

### è¨“ç·´å„ªåŒ–
- **torch.compile()**ï¼šç´„20-30% æ›´å¿«çš„è¨“ç·´
- **æ··åˆç²¾åº¦**ï¼šFP16 è¨“ç·´ä»¥ç¯€çœè¨˜æ†¶é«”
- **æ¢¯åº¦è£å‰ª**ï¼šç©©å®šçš„æ¢¯åº¦è£å‰ªè¨“ç·´
- **å­¸ç¿’ç‡èª¿åº¦**ï¼šè‡ªé©æ‡‰å­¸ç¿’ç‡èª¿åº¦

## ğŸ”§ é…ç½®

### è¨“ç·´é…ç½®ï¼ˆ`config_quantization_fixed.json`ï¼‰
```json
{
    "model_name": "HybridKeypointNet",
    "model_save_path": "models/keypoint_model_vit_post",
    "pretrained_model_path": "models/keypoint_model_vit.pth",
    "yolo_model_path": "models/yolo_finetuned/best.pt",
    "keypoints_data_src": "via_proj/via_project_22Aug2025_16h07m06s.json",
    "image_path": "RGB-images-jpg/",
    "allowed_classes": [1],
    "batch_size": 16,
    "learning_rate": 0.001,
    "num_epochs": 50,
    "use_augmentation": true,
    "use_mixup": true,
    "early_stopping_patience": 10
}
```

## ğŸ“ˆ è¨“ç·´æµç¨‹

### æ•¸æ“šæº–å‚™
1. **çœŸå¯¦æ•¸æ“š**ï¼šä½¿ç”¨ VIA å·¥å…·æ¨™è¨»çš„åºŠå–®åœ–åƒå’Œé—œéµé»
2. **åˆæˆæ•¸æ“š**ï¼šä½¿ç”¨ Blender ç”Ÿæˆçš„è®Šå½¢å¸ƒæ–™æ•¸æ“š
3. **æ•¸æ“šå¢å¼·**ï¼šæ—‹è½‰ã€ç¿»è½‰ã€é¡è‰²è®ŠåŒ–ç­‰

### ç¬¬ä¸€éšæ®µï¼šé è¨“ç·´
1. **æ•¸æ“šè¼‰å…¥**ï¼šè¼‰å…¥åœ–åƒå’Œé—œéµé»è¨»è§£
2. **YOLO åˆ†å‰²**ï¼šä½¿ç”¨å¾®èª¿çš„ YOLO æå–åºŠå–®é®ç½©
3. **æ¨¡å‹è¨“ç·´**ï¼šä½¿ç”¨åŸºæœ¬å„ªåŒ–å¾é ­é–‹å§‹è¨“ç·´
4. **æ¨¡å‹å„²å­˜**ï¼šå„²å­˜é è¨“ç·´æ¨¡å‹ä¾›å¾Œè¨“ç·´ä½¿ç”¨

### ç¬¬äºŒéšæ®µï¼šå¾Œè¨“ç·´
1. **è¼‰å…¥é è¨“ç·´æ¨¡å‹**ï¼šå¾ç¬¬ä¸€éšæ®µçµæœè¼‰å…¥
2. **é€²éšå¢å¼·**ï¼šæ‡‰ç”¨æ—‹è½‰ã€ç¿»è½‰å’Œ mixup
3. **å„ªåŒ–è¨“ç·´**ï¼šä½¿ç”¨ torch.compile() å’Œæ—©åœæ©Ÿåˆ¶è¨“ç·´
4. **è©•ä¼°**ï¼šåœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°ä¸¦è¦–è¦ºåŒ–çµæœ

## ğŸš€ éƒ¨ç½²

### ç”Ÿç”¢éƒ¨ç½²
1. **å®Œæˆç¬¬ä¸€éšæ®µ**ï¼šé è¨“ç·´æ¨¡å‹
2. **å®Œæˆç¬¬äºŒéšæ®µ**ï¼šå¾Œè¨“ç·´å„ªåŒ–
3. **éƒ¨ç½²æ¨¡å‹**ï¼šä½¿ç”¨æœ€çµ‚å„ªåŒ–æ¨¡å‹é€²è¡Œæ¨ç†

### å³æ™‚æ¨ç†
```python
import torch
from src.models import HybridKeypointNet

# è¼‰å…¥è¨“ç·´æ¨¡å‹
model = HybridKeypointNet(...)
model.load_state_dict(torch.load("models/keypoint_model_vit_post.pth"))
model.eval()

# åŸ·è¡Œæ¨ç†
with torch.no_grad():
    output = model(input_tensor)
```

## ğŸ“ ä½¿ç”¨ç¯„ä¾‹

### å®Œæ•´è¨“ç·´å·¥ä½œæµç¨‹
```bash
# æ­¥é©Ÿ0ï¼šç”Ÿæˆåˆæˆæ•¸æ“šï¼ˆå¯é¸ï¼‰
cd cloth_data_gen
blender --background --python cloth_dataset_gen.py

# æ­¥é©Ÿ1ï¼šé è¨“ç·´
python keypoint_detection_model_training.py

# æ­¥é©Ÿ2ï¼šå¾Œè¨“ç·´å„ªåŒ–
python post_keypoint_detection_model_training.py config_quantization_fixed.json
```

### è‡ªè¨‚é…ç½®
```python
# æ ¹æ“šéœ€æ±‚ä¿®æ”¹ config_quantization_fixed.json
{
    "num_epochs": 100,
    "batch_size": 32,
    "learning_rate": 0.0005,
    "early_stopping_patience": 20
}
```

## ğŸ”® æœªä¾†æ”¹é€²

### è¨ˆåŠƒåŠŸèƒ½
- **TensorRT å„ªåŒ–**ï¼šä½¿ç”¨ TensorRT è½‰æ›å¯¦ç¾2-5å€æ›´å¿«çš„æ¨ç†
- **é‡åŒ–æ”¯æ´**ï¼šINT8 é‡åŒ–ç”¨æ–¼é‚Šç·£éƒ¨ç½²
- **æ¨¡å‹åŒ¯å‡º**ï¼šONNX å’Œ TorchScript åŒ¯å‡ºåŠŸèƒ½
- **é€²éšå¢å¼·**ï¼šæ›´è¤‡é›œçš„æ•¸æ“šå¢å¼·ç­–ç•¥
- **ä¸»å‹•å­¸ç¿’**ï¼šä¸ç¢ºå®šæ€§æ¡æ¨£ç”¨æ–¼é«˜æ•ˆè¨“ç·´

### TensorRT æ•´åˆï¼ˆæœªä¾†ï¼‰
```bash
# è½‰æ›ç‚º TensorRT ä»¥å„ªåŒ–æ¨ç†
python convert_to_tensorrt.py \
    --model_path models/keypoint_model_vit_post.pth \
    --precision fp16 \
    --test_inference

# æ•ˆèƒ½åŸºæº–æ¸¬è©¦
python test_tensorrt_inference.py \
    --pytorch_model models/keypoint_model_vit_post.pth \
    --tensorrt_model models/keypoint_model_vit_post.trt
```

## ğŸ“„ æˆæ¬Š

æœ¬å°ˆæ¡ˆæ¡ç”¨ MIT æˆæ¬Šæ¢æ¬¾ - è©³è¦‹ LICENSE æª”æ¡ˆã€‚

## ğŸ™ è‡´è¬

- YOLO æ¶æ§‹ç”± Ultralytics æä¾›
- Vision Transformer ç”± Google Research æä¾›
- PyTorch ç”± Facebook Research æä¾›
